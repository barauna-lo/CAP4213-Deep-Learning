{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch MLP - Aprendizado Profundo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barauna-lo/CAP4213-Deep-Learning/blob/main/PyTorch_MLP_Aprendizado_Profundo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1at01NiwHKk1"
      },
      "source": [
        "<img src=https://raw.githubusercontent.com/barauna-lo/CAP4213-Deep-Learning/main/logoinpe.png>\n",
        "\n",
        "\n",
        "# CAP-421-3: Aprendizado Profundo\n",
        "\n",
        "## Atividade 2: Exemplo de aplicação em Python\n",
        "\n",
        "* Luan Orion Baraúna \n",
        "\n",
        "* Renato Maximiniano \n",
        "\n",
        "* Vinicius Monego"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLMNP0h-HnUb"
      },
      "source": [
        "# Introdução\n",
        "\n",
        "A aplicação deste trabalho é em previsão do tempo, mais especificamente previsão de precipitação, utilizando redes neurais artificiais do tipo MLP (Multi Layer Perceptron) como alternativa à previsão numérica do tempo (NWP). A NWP possui um custo computacional muito alto, e o aprendizado profundo apresenta a vantagem de execução mais eficiente em comparação à NWP.\n",
        "\n",
        "Este trabalho foi implementado e apresentado pelo aluno [Vinicius Monego](http://lattes.cnpq.br/1175145401785658) na disciplina de Inteligência Artificial da CAP/INPE em 2021. A implementação da rede neural foi realizada no framework [PyTorch](https://pytorch.org/) puro, sem uso de interfaces.\n",
        "\n",
        "## Importando as Bibliotecas\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png\" width = 300 >\n",
        "\n",
        "\n",
        "<img src='https://pandas.pydata.org/static/img/pandas_white.svg  ' width = 300>\n",
        "\n",
        "<img src='https://user-images.githubusercontent.com/1217238/65354639-dd928f80-dba4-11e9-833b-bc3e8c6a737d.png' width = 300>\n",
        "\n",
        "<img src='https://scipy-lectures.org/_images/scikit-learn-logo.png' width =300>\n",
        "\n",
        "<img src='https://matplotlib.org/stable/_static/logo2_compressed.svg' width =300>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4XMSaIerRSB",
        "outputId": "74a76a1a-b769-491b-c838-dad8fe97e9fe"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as nnf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kzlSxrnyMuK"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noRK53DBGsW2"
      },
      "source": [
        "# Garantir a reprodutibilidade da rede\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.use_deterministic_algorithms(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSAdgqIYJKIl"
      },
      "source": [
        "Os dados utilizados são do [GPCP](https://psl.noaa.gov/data/gridded/data.gpcp.html) (Global Precipitation Climate Project), que contém diferentes tipos de dados climáticos (vento, precipitação, humidade, geopotencial, temperatura e pressão) coletados mensalmente de 1979 a 2020 em diferentes pontos de grade, identificados pelas coordenadas de latitude e longitude. O objetivo da rede é aprender com estes dados para estimar o nível acumulado de precipitação do mês seguinte.\n",
        "\n",
        "Os dados foram convertidos do formato original (NetCDF) para uma panilha XLSX para facilitar a manipulação. A região extraída dos dados é a América do Sul.\n",
        "\n",
        "<centet> <img src=\"https://psl.noaa.gov/data/gridded/images/small/gpcp.png\" alt=\"centered image\" width = 600> </centet>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdYGuMDF3tqR"
      },
      "source": [
        "weather_data = pd.read_excel('/content/drive/My Drive/dados-outono.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEwyPJ1mJ5LY"
      },
      "source": [
        "Os dados como estão na planilha não estão normalizados. Os Eles precisam ser normalizados porque as funções de ativação dependem do dado normalizado para introduzir a não-linearidade na rede.\n",
        "\n",
        "Para normalização, é utilizado o pacote `scikit-learn` e seu recurso de normalização do tipo Min-Max."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkY-eNiEPhtO",
        "outputId": "765175c2-9627-4324-b6b4-c46067dcb1df"
      },
      "source": [
        "x_scaler = MinMaxScaler()\n",
        "x_scaler.fit(weather_data.loc[:, 'v850':'prec_GPCP'])\n",
        "\n",
        "y_scaler = MinMaxScaler()\n",
        "y_scaler.fit(weather_data.loc[:, 'prec_GPCP'].to_numpy().reshape(-1, 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MinMaxScaler(copy=True, feature_range=(0, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgCNDVhwKvz-"
      },
      "source": [
        "O treinamento de um modelo de machine learning normalmente possui três tipos de conjuntos de dados:\n",
        "\n",
        "- **Treinamento**: Parte dos dados que influenciam no diretamente no ajuste dos pesos. \n",
        "- **Validação**: Parte dos dados da qual se calculam as métricas para saber se a rede está aprendendo. Faz parte do treinamento, mas não influencia no ajuste dos pesos.\n",
        "- **Teste**: Não faz parte do conjunto de treinamento. É a parte dos dados que a rede não viu durante o treinamento e serve para avaliar a rede já treinada.\n",
        "\n",
        "O conjunto de treinamento+validação é escolhido como sendo abaixo de 2017, enquanto o de teste de 2017 e após."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g69xuich3yyv"
      },
      "source": [
        "trainval_data = weather_data.loc[weather_data['year'] < 2017]\n",
        "test_data = weather_data.loc[weather_data['year'] >= 2017]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tApzWhYFLxLj"
      },
      "source": [
        "A classe que prepara os dados (leitura e processamento dos dados, normalização e seleção das *features*) é vista abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZcKfGWsrkp2"
      },
      "source": [
        "class GPCP(Dataset):\n",
        "\n",
        "  def __init__(self, data, transform=None):\n",
        "    \"\"\"Read the dataset and assign varbiales\"\"\"\n",
        "    \n",
        "    self._data = data\n",
        "    self.x, self.y = self._prepare_data(self._data)\n",
        "\n",
        "    self.x = torch.from_numpy(self.x).float().to(device)\n",
        "    self.y = torch.from_numpy(np.expand_dims(self.y, axis=1)).float().to(device)\n",
        "    self._n_samples = self.x.shape[0]\n",
        "\n",
        "    self._transform = transform\n",
        "\n",
        "  def _prepare_data(self, data):\n",
        "    \"\"\"Sort data by latitude/longitude, select features\"\"\"\n",
        "    data_group = [pd.DataFrame(y) for x, y in data.groupby(['lat', 'lon'], as_index=False)]\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for df in data_group:\n",
        "      x.append(df.loc[:, 'v850':'prec_GPCP'][:-1].to_numpy())\n",
        "      y.append(df.loc[:, 'prec_GPCP'][1:].to_numpy())\n",
        "\n",
        "    x = np.array(x)\n",
        "    xshp = x.shape\n",
        "    x = x.reshape((xshp[0]*xshp[1], xshp[2]))\n",
        "\n",
        "    y = np.array(y)\n",
        "    yshp = y.shape\n",
        "    y = y.reshape((yshp[0] * yshp[1]))\n",
        "\n",
        "    return x, y\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"Index the dataset\"\"\"\n",
        "    sample = self.x[index], self.y[index]\n",
        "\n",
        "    if self._transform:\n",
        "      sample = self._transform(sample)\n",
        "\n",
        "    return sample\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"Return len()\"\"\"\n",
        "    return self._n_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_abiUapzMrtQ"
      },
      "source": [
        "E a transformação do PyTorch que normaliza os dados e é passada como parâmetro para a classe acima é vista abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvhREr4rK8Zy"
      },
      "source": [
        "class NormalizeTransform:\n",
        "  \"\"\"Normalize the dataset\"\"\"\n",
        "  def __init__(self, x_scaler, y_scaler):\n",
        "    self._x_scaler = x_scaler\n",
        "    self._y_scaler = y_scaler\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    sample_x = np.expand_dims(sample[0].cpu(), axis=0)\n",
        "    sample_y = sample[1].cpu().reshape(-1, 1)\n",
        "    x_normalized = self._x_scaler.transform(sample_x)\n",
        "    y_normalized = self._y_scaler.transform(sample_y)\n",
        "    x_normalized = torch.from_numpy(x_normalized).float().to(device)\n",
        "    y_normalized = torch.from_numpy(y_normalized).float().to(device)\n",
        "    return x_normalized, y_normalized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc0hvk8WNIiV"
      },
      "source": [
        "Na próxima célula ocorre a preparação (seleção das *features* e normalização) do conjunto de teste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_KHSGlW4v75"
      },
      "source": [
        "dataset = GPCP(trainval_data, transform=NormalizeTransform(x_scaler, y_scaler))\n",
        "\n",
        "test_data_input = test_data.loc[:, 'v850':'prec_GPCP']\n",
        "test_dataset = y_scaler.transform(test_data_input)\n",
        "# test_dataset = test_dataset.to_numpy()\n",
        "test_dataset = torch.from_numpy(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZeXXr5QNS_U"
      },
      "source": [
        "Na próxima célula ocorre a separação do conjunto de treinamento entre os dados e treinamento e os dados de validação (80% e 20%, respectivamente)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wbOvKLxrloz"
      },
      "source": [
        "# split the dataset into train, validation and test sets\n",
        "\n",
        "total_count = len(dataset)\n",
        "\n",
        "train_count = int(0.8 * total_count)\n",
        "valid_count = int(0.2 * total_count)\n",
        "\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(\n",
        "    dataset, (train_count, valid_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q93EMcRNcjz"
      },
      "source": [
        "Na próxima célula ocorre o carregamento dos dados de treinamento para um tipo de dado que é usado pelo PyTorch para treinar a rede (DataLoader)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pyq13KNEViM"
      },
      "source": [
        "# load the dataset\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCHAg8_HN64r"
      },
      "source": [
        "Na próxima célula ocorre a definição da arquitetura da rede neural. É uma rede de 2 camadas escondidas, com 30 e 35 neurônios respectivamente, utilizando a função ReLU parametrizada como função de ativação e a função Sigmoide na camada de saída."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnBnn6UMroCa"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size):\n",
        "    super(MLP, self).__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(input_size, 30),\n",
        "      nn.PReLU(),\n",
        "      nn.Linear(30, 35),\n",
        "      nn.PReLU(),\n",
        "      nn.Linear(35, 1),\n",
        "      nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-mLTVN5Odei"
      },
      "source": [
        "Após, o modelo é criado utilizando a função de erro médio quadrático como função custo e o otimizador Adadelta como função de otimização. Também é escolhido o número de épocas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9LhuXjXrrsd"
      },
      "source": [
        "model = MLP(9).to(device)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adadelta(model.parameters())\n",
        "num_epochs = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxaWg4WnOuZb"
      },
      "source": [
        "A próxima célula é o treinamento da rede. Ao final, o objeto `model` será a rede treinada e poderá ser usada para avaliar o conjunto de teste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ-mv1o-rsyE",
        "outputId": "2e3008ad-1530-496f-a6e1-b283b69bcd4d"
      },
      "source": [
        "# train stage\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss = 0.0\n",
        "  valid_loss = 0.0\n",
        "  for i, (x, y) in enumerate(train_dataloader):\n",
        "    model.train()\n",
        "\n",
        "    # forward pass\n",
        "    output = model(x)\n",
        "    loss = loss_function(output, y)\n",
        "\n",
        "    # backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item() * x.size(0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, (x, y) in enumerate(valid_dataloader):\n",
        "      model.eval()\n",
        "\n",
        "      output = model(x)\n",
        "      loss = (loss_function(output, y))\n",
        "\n",
        "      valid_loss += loss.item() * x.size(0)\n",
        "\n",
        "  train_loss /= len(train_dataset)\n",
        "  valid_loss /= len(valid_dataset)\n",
        "\n",
        "  print(f\"Epoch: {epoch+1}. Train loss: {train_loss:.8f}. Validation loss: {valid_loss:.8f}\")\n",
        "\n",
        "  train_losses.append(train_loss)\n",
        "  valid_losses.append(valid_loss)\n",
        "\n",
        "train_losses = np.array(train_losses)\n",
        "valid_losses = np.array(valid_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Epoch: 1. Train loss: 0.00803209. Validation loss: 0.00553512\n",
            "Epoch: 2. Train loss: 0.00563681. Validation loss: 0.00515575\n",
            "Epoch: 3. Train loss: 0.00545424. Validation loss: 0.00519990\n",
            "Epoch: 4. Train loss: 0.00535168. Validation loss: 0.00548680\n",
            "Epoch: 5. Train loss: 0.00531450. Validation loss: 0.00496251\n",
            "Epoch: 6. Train loss: 0.00525262. Validation loss: 0.00492629\n",
            "Epoch: 7. Train loss: 0.00521796. Validation loss: 0.00538865\n",
            "Epoch: 8. Train loss: 0.00518553. Validation loss: 0.00507075\n",
            "Epoch: 9. Train loss: 0.00517002. Validation loss: 0.00496628\n",
            "Epoch: 10. Train loss: 0.00514838. Validation loss: 0.00498768\n",
            "Epoch: 11. Train loss: 0.00514907. Validation loss: 0.00497278\n",
            "Epoch: 12. Train loss: 0.00512823. Validation loss: 0.00517916\n",
            "Epoch: 13. Train loss: 0.00513116. Validation loss: 0.00494326\n",
            "Epoch: 14. Train loss: 0.00511223. Validation loss: 0.00492396\n",
            "Epoch: 15. Train loss: 0.00511568. Validation loss: 0.00491631\n",
            "Epoch: 16. Train loss: 0.00507333. Validation loss: 0.00511644\n",
            "Epoch: 17. Train loss: 0.00509223. Validation loss: 0.00494574\n",
            "Epoch: 18. Train loss: 0.00507411. Validation loss: 0.00531327\n",
            "Epoch: 19. Train loss: 0.00505688. Validation loss: 0.00533134\n",
            "Epoch: 20. Train loss: 0.00505116. Validation loss: 0.00495302\n",
            "Epoch: 21. Train loss: 0.00505128. Validation loss: 0.00493906\n",
            "Epoch: 22. Train loss: 0.00504251. Validation loss: 0.00484375\n",
            "Epoch: 23. Train loss: 0.00505144. Validation loss: 0.00490666\n",
            "Epoch: 24. Train loss: 0.00502808. Validation loss: 0.00487599\n",
            "Epoch: 25. Train loss: 0.00501177. Validation loss: 0.00481786\n",
            "Epoch: 26. Train loss: 0.00501051. Validation loss: 0.00488401\n",
            "Epoch: 27. Train loss: 0.00501097. Validation loss: 0.00483288\n",
            "Epoch: 28. Train loss: 0.00501789. Validation loss: 0.00483081\n",
            "Epoch: 29. Train loss: 0.00499832. Validation loss: 0.00486579\n",
            "Epoch: 30. Train loss: 0.00498812. Validation loss: 0.00501120\n",
            "Epoch: 31. Train loss: 0.00500104. Validation loss: 0.00494373\n",
            "Epoch: 32. Train loss: 0.00499195. Validation loss: 0.00489814\n",
            "Epoch: 33. Train loss: 0.00496420. Validation loss: 0.00482593\n",
            "Epoch: 34. Train loss: 0.00497600. Validation loss: 0.00478849\n",
            "Epoch: 35. Train loss: 0.00496721. Validation loss: 0.00492506\n",
            "Epoch: 36. Train loss: 0.00495839. Validation loss: 0.00487737\n",
            "Epoch: 37. Train loss: 0.00495331. Validation loss: 0.00499593\n",
            "Epoch: 38. Train loss: 0.00495695. Validation loss: 0.00481083\n",
            "Epoch: 39. Train loss: 0.00495744. Validation loss: 0.00477130\n",
            "Epoch: 40. Train loss: 0.00495314. Validation loss: 0.00482931\n",
            "Epoch: 41. Train loss: 0.00494657. Validation loss: 0.00512017\n",
            "Epoch: 42. Train loss: 0.00493276. Validation loss: 0.00480643\n",
            "Epoch: 43. Train loss: 0.00494045. Validation loss: 0.00475047\n",
            "Epoch: 44. Train loss: 0.00493296. Validation loss: 0.00504578\n",
            "Epoch: 45. Train loss: 0.00491646. Validation loss: 0.00478851\n",
            "Epoch: 46. Train loss: 0.00492821. Validation loss: 0.00479939\n",
            "Epoch: 47. Train loss: 0.00490956. Validation loss: 0.00484400\n",
            "Epoch: 48. Train loss: 0.00491199. Validation loss: 0.00505297\n",
            "Epoch: 49. Train loss: 0.00490631. Validation loss: 0.00471632\n",
            "Epoch: 50. Train loss: 0.00489111. Validation loss: 0.00478580\n",
            "Epoch: 51. Train loss: 0.00490733. Validation loss: 0.00489874\n",
            "Epoch: 52. Train loss: 0.00488580. Validation loss: 0.00484476\n",
            "Epoch: 53. Train loss: 0.00489545. Validation loss: 0.00493334\n",
            "Epoch: 54. Train loss: 0.00487650. Validation loss: 0.00490037\n",
            "Epoch: 55. Train loss: 0.00489340. Validation loss: 0.00473928\n",
            "Epoch: 56. Train loss: 0.00488395. Validation loss: 0.00471731\n",
            "Epoch: 57. Train loss: 0.00486731. Validation loss: 0.00473270\n",
            "Epoch: 58. Train loss: 0.00487162. Validation loss: 0.00480066\n",
            "Epoch: 59. Train loss: 0.00484113. Validation loss: 0.00473900\n",
            "Epoch: 60. Train loss: 0.00486939. Validation loss: 0.00467864\n",
            "Epoch: 61. Train loss: 0.00486253. Validation loss: 0.00482368\n",
            "Epoch: 62. Train loss: 0.00483865. Validation loss: 0.00507390\n",
            "Epoch: 63. Train loss: 0.00483534. Validation loss: 0.00492999\n",
            "Epoch: 64. Train loss: 0.00484901. Validation loss: 0.00471580\n",
            "Epoch: 65. Train loss: 0.00482648. Validation loss: 0.00470328\n",
            "Epoch: 66. Train loss: 0.00481675. Validation loss: 0.00471970\n",
            "Epoch: 67. Train loss: 0.00483400. Validation loss: 0.00482483\n",
            "Epoch: 68. Train loss: 0.00482205. Validation loss: 0.00493143\n",
            "Epoch: 69. Train loss: 0.00481261. Validation loss: 0.00495300\n",
            "Epoch: 70. Train loss: 0.00483416. Validation loss: 0.00467367\n",
            "Epoch: 71. Train loss: 0.00481480. Validation loss: 0.00474120\n",
            "Epoch: 72. Train loss: 0.00480188. Validation loss: 0.00485231\n",
            "Epoch: 73. Train loss: 0.00480923. Validation loss: 0.00470126\n",
            "Epoch: 74. Train loss: 0.00481797. Validation loss: 0.00476423\n",
            "Epoch: 75. Train loss: 0.00480596. Validation loss: 0.00494222\n",
            "Epoch: 76. Train loss: 0.00480086. Validation loss: 0.00468528\n",
            "Epoch: 77. Train loss: 0.00478673. Validation loss: 0.00477945\n",
            "Epoch: 78. Train loss: 0.00479289. Validation loss: 0.00464863\n",
            "Epoch: 79. Train loss: 0.00478745. Validation loss: 0.00488414\n",
            "Epoch: 80. Train loss: 0.00478450. Validation loss: 0.00483626\n",
            "Epoch: 81. Train loss: 0.00478776. Validation loss: 0.00480730\n",
            "Epoch: 82. Train loss: 0.00477466. Validation loss: 0.00501354\n",
            "Epoch: 83. Train loss: 0.00476984. Validation loss: 0.00487066\n",
            "Epoch: 84. Train loss: 0.00477438. Validation loss: 0.00474062\n",
            "Epoch: 85. Train loss: 0.00476166. Validation loss: 0.00476801\n",
            "Epoch: 86. Train loss: 0.00475863. Validation loss: 0.00491827\n",
            "Epoch: 87. Train loss: 0.00476090. Validation loss: 0.00474525\n",
            "Epoch: 88. Train loss: 0.00476295. Validation loss: 0.00479179\n",
            "Epoch: 89. Train loss: 0.00474377. Validation loss: 0.00479625\n",
            "Epoch: 90. Train loss: 0.00473398. Validation loss: 0.00476482\n",
            "Epoch: 91. Train loss: 0.00475362. Validation loss: 0.00459691\n",
            "Epoch: 92. Train loss: 0.00474923. Validation loss: 0.00466970\n",
            "Epoch: 93. Train loss: 0.00474189. Validation loss: 0.00463538\n",
            "Epoch: 94. Train loss: 0.00473577. Validation loss: 0.00476954\n",
            "Epoch: 95. Train loss: 0.00474695. Validation loss: 0.00488206\n",
            "Epoch: 96. Train loss: 0.00474798. Validation loss: 0.00474684\n",
            "Epoch: 97. Train loss: 0.00472294. Validation loss: 0.00467527\n",
            "Epoch: 98. Train loss: 0.00470863. Validation loss: 0.00466833\n",
            "Epoch: 99. Train loss: 0.00472670. Validation loss: 0.00469553\n",
            "Epoch: 100. Train loss: 0.00471978. Validation loss: 0.00464451\n",
            "Epoch: 101. Train loss: 0.00472280. Validation loss: 0.00461710\n",
            "Epoch: 102. Train loss: 0.00472310. Validation loss: 0.00475506\n",
            "Epoch: 103. Train loss: 0.00471804. Validation loss: 0.00478974\n",
            "Epoch: 104. Train loss: 0.00471480. Validation loss: 0.00465628\n",
            "Epoch: 105. Train loss: 0.00472231. Validation loss: 0.00464950\n",
            "Epoch: 106. Train loss: 0.00470388. Validation loss: 0.00465901\n",
            "Epoch: 107. Train loss: 0.00469166. Validation loss: 0.00472267\n",
            "Epoch: 108. Train loss: 0.00471223. Validation loss: 0.00465671\n",
            "Epoch: 109. Train loss: 0.00470261. Validation loss: 0.00466143\n",
            "Epoch: 110. Train loss: 0.00468584. Validation loss: 0.00465551\n",
            "Epoch: 111. Train loss: 0.00469362. Validation loss: 0.00480462\n",
            "Epoch: 112. Train loss: 0.00469838. Validation loss: 0.00476671\n",
            "Epoch: 113. Train loss: 0.00468541. Validation loss: 0.00469690\n",
            "Epoch: 114. Train loss: 0.00468244. Validation loss: 0.00473048\n",
            "Epoch: 115. Train loss: 0.00469571. Validation loss: 0.00462262\n",
            "Epoch: 116. Train loss: 0.00469878. Validation loss: 0.00482563\n",
            "Epoch: 117. Train loss: 0.00469776. Validation loss: 0.00468656\n",
            "Epoch: 118. Train loss: 0.00466869. Validation loss: 0.00463874\n",
            "Epoch: 119. Train loss: 0.00468955. Validation loss: 0.00471012\n",
            "Epoch: 120. Train loss: 0.00467032. Validation loss: 0.00460877\n",
            "Epoch: 121. Train loss: 0.00468524. Validation loss: 0.00466775\n",
            "Epoch: 122. Train loss: 0.00466881. Validation loss: 0.00473738\n",
            "Epoch: 123. Train loss: 0.00468304. Validation loss: 0.00476730\n",
            "Epoch: 124. Train loss: 0.00465729. Validation loss: 0.00459019\n",
            "Epoch: 125. Train loss: 0.00467013. Validation loss: 0.00467335\n",
            "Epoch: 126. Train loss: 0.00466047. Validation loss: 0.00489573\n",
            "Epoch: 127. Train loss: 0.00467588. Validation loss: 0.00468456\n",
            "Epoch: 128. Train loss: 0.00466028. Validation loss: 0.00468440\n",
            "Epoch: 129. Train loss: 0.00464444. Validation loss: 0.00472056\n",
            "Epoch: 130. Train loss: 0.00465376. Validation loss: 0.00470434\n",
            "Epoch: 131. Train loss: 0.00465485. Validation loss: 0.00459292\n",
            "Epoch: 132. Train loss: 0.00467086. Validation loss: 0.00469076\n",
            "Epoch: 133. Train loss: 0.00464436. Validation loss: 0.00467987\n",
            "Epoch: 134. Train loss: 0.00465303. Validation loss: 0.00483726\n",
            "Epoch: 135. Train loss: 0.00465361. Validation loss: 0.00472418\n",
            "Epoch: 136. Train loss: 0.00464188. Validation loss: 0.00459011\n",
            "Epoch: 137. Train loss: 0.00466113. Validation loss: 0.00470429\n",
            "Epoch: 138. Train loss: 0.00464618. Validation loss: 0.00468622\n",
            "Epoch: 139. Train loss: 0.00464135. Validation loss: 0.00468742\n",
            "Epoch: 140. Train loss: 0.00463127. Validation loss: 0.00487819\n",
            "Epoch: 141. Train loss: 0.00464297. Validation loss: 0.00460881\n",
            "Epoch: 142. Train loss: 0.00465464. Validation loss: 0.00486977\n",
            "Epoch: 143. Train loss: 0.00463448. Validation loss: 0.00473080\n",
            "Epoch: 144. Train loss: 0.00465009. Validation loss: 0.00477205\n",
            "Epoch: 145. Train loss: 0.00463675. Validation loss: 0.00471016\n",
            "Epoch: 146. Train loss: 0.00464238. Validation loss: 0.00479178\n",
            "Epoch: 147. Train loss: 0.00462216. Validation loss: 0.00478061\n",
            "Epoch: 148. Train loss: 0.00462873. Validation loss: 0.00474310\n",
            "Epoch: 149. Train loss: 0.00463339. Validation loss: 0.00484858\n",
            "Epoch: 150. Train loss: 0.00461697. Validation loss: 0.00474868\n",
            "Epoch: 151. Train loss: 0.00462528. Validation loss: 0.00474159\n",
            "Epoch: 152. Train loss: 0.00464110. Validation loss: 0.00485604\n",
            "Epoch: 153. Train loss: 0.00465325. Validation loss: 0.00457867\n",
            "Epoch: 154. Train loss: 0.00462468. Validation loss: 0.00458390\n",
            "Epoch: 155. Train loss: 0.00464046. Validation loss: 0.00473498\n",
            "Epoch: 156. Train loss: 0.00463954. Validation loss: 0.00489562\n",
            "Epoch: 157. Train loss: 0.00462048. Validation loss: 0.00483523\n",
            "Epoch: 158. Train loss: 0.00460745. Validation loss: 0.00471644\n",
            "Epoch: 159. Train loss: 0.00461291. Validation loss: 0.00473079\n",
            "Epoch: 160. Train loss: 0.00462668. Validation loss: 0.00468957\n",
            "Epoch: 161. Train loss: 0.00460232. Validation loss: 0.00479680\n",
            "Epoch: 162. Train loss: 0.00462310. Validation loss: 0.00472166\n",
            "Epoch: 163. Train loss: 0.00462240. Validation loss: 0.00467362\n",
            "Epoch: 164. Train loss: 0.00461313. Validation loss: 0.00464627\n",
            "Epoch: 165. Train loss: 0.00460836. Validation loss: 0.00473656\n",
            "Epoch: 166. Train loss: 0.00458288. Validation loss: 0.00470181\n",
            "Epoch: 167. Train loss: 0.00461148. Validation loss: 0.00467553\n",
            "Epoch: 168. Train loss: 0.00459409. Validation loss: 0.00462372\n",
            "Epoch: 169. Train loss: 0.00458792. Validation loss: 0.00469929\n",
            "Epoch: 170. Train loss: 0.00460174. Validation loss: 0.00461213\n",
            "Epoch: 171. Train loss: 0.00459734. Validation loss: 0.00458006\n",
            "Epoch: 172. Train loss: 0.00458520. Validation loss: 0.00459796\n",
            "Epoch: 173. Train loss: 0.00458403. Validation loss: 0.00463096\n",
            "Epoch: 174. Train loss: 0.00458684. Validation loss: 0.00455804\n",
            "Epoch: 175. Train loss: 0.00458039. Validation loss: 0.00461792\n",
            "Epoch: 176. Train loss: 0.00455972. Validation loss: 0.00468697\n",
            "Epoch: 177. Train loss: 0.00459013. Validation loss: 0.00457017\n",
            "Epoch: 178. Train loss: 0.00458132. Validation loss: 0.00477748\n",
            "Epoch: 179. Train loss: 0.00458481. Validation loss: 0.00460357\n",
            "Epoch: 180. Train loss: 0.00458171. Validation loss: 0.00471741\n",
            "Epoch: 181. Train loss: 0.00457019. Validation loss: 0.00462200\n",
            "Epoch: 182. Train loss: 0.00455367. Validation loss: 0.00469527\n",
            "Epoch: 183. Train loss: 0.00457976. Validation loss: 0.00477833\n",
            "Epoch: 184. Train loss: 0.00456915. Validation loss: 0.00459802\n",
            "Epoch: 185. Train loss: 0.00454656. Validation loss: 0.00479798\n",
            "Epoch: 186. Train loss: 0.00455719. Validation loss: 0.00472090\n",
            "Epoch: 187. Train loss: 0.00455708. Validation loss: 0.00476078\n",
            "Epoch: 188. Train loss: 0.00456248. Validation loss: 0.00477657\n",
            "Epoch: 189. Train loss: 0.00455626. Validation loss: 0.00462337\n",
            "Epoch: 190. Train loss: 0.00453802. Validation loss: 0.00475313\n",
            "Epoch: 191. Train loss: 0.00455425. Validation loss: 0.00462439\n",
            "Epoch: 192. Train loss: 0.00455318. Validation loss: 0.00466811\n",
            "Epoch: 193. Train loss: 0.00454637. Validation loss: 0.00457748\n",
            "Epoch: 194. Train loss: 0.00453866. Validation loss: 0.00458036\n",
            "Epoch: 195. Train loss: 0.00453565. Validation loss: 0.00489936\n",
            "Epoch: 196. Train loss: 0.00456080. Validation loss: 0.00459620\n",
            "Epoch: 197. Train loss: 0.00453000. Validation loss: 0.00474291\n",
            "Epoch: 198. Train loss: 0.00455006. Validation loss: 0.00466890\n",
            "Epoch: 199. Train loss: 0.00454568. Validation loss: 0.00471262\n",
            "Epoch: 200. Train loss: 0.00452283. Validation loss: 0.00472066\n",
            "Epoch: 201. Train loss: 0.00450254. Validation loss: 0.00456845\n",
            "Epoch: 202. Train loss: 0.00451588. Validation loss: 0.00472985\n",
            "Epoch: 203. Train loss: 0.00453579. Validation loss: 0.00463888\n",
            "Epoch: 204. Train loss: 0.00451937. Validation loss: 0.00455099\n",
            "Epoch: 205. Train loss: 0.00453250. Validation loss: 0.00484569\n",
            "Epoch: 206. Train loss: 0.00452030. Validation loss: 0.00458908\n",
            "Epoch: 207. Train loss: 0.00452815. Validation loss: 0.00459773\n",
            "Epoch: 208. Train loss: 0.00452385. Validation loss: 0.00469699\n",
            "Epoch: 209. Train loss: 0.00451301. Validation loss: 0.00483736\n",
            "Epoch: 210. Train loss: 0.00452114. Validation loss: 0.00462964\n",
            "Epoch: 211. Train loss: 0.00453055. Validation loss: 0.00465361\n",
            "Epoch: 212. Train loss: 0.00451761. Validation loss: 0.00470223\n",
            "Epoch: 213. Train loss: 0.00451217. Validation loss: 0.00461983\n",
            "Epoch: 214. Train loss: 0.00449675. Validation loss: 0.00478256\n",
            "Epoch: 215. Train loss: 0.00449850. Validation loss: 0.00459420\n",
            "Epoch: 216. Train loss: 0.00450557. Validation loss: 0.00455770\n",
            "Epoch: 217. Train loss: 0.00448768. Validation loss: 0.00465740\n",
            "Epoch: 218. Train loss: 0.00449789. Validation loss: 0.00467045\n",
            "Epoch: 219. Train loss: 0.00447592. Validation loss: 0.00464914\n",
            "Epoch: 220. Train loss: 0.00450279. Validation loss: 0.00454955\n",
            "Epoch: 221. Train loss: 0.00451693. Validation loss: 0.00457573\n",
            "Epoch: 222. Train loss: 0.00447224. Validation loss: 0.00470379\n",
            "Epoch: 223. Train loss: 0.00447799. Validation loss: 0.00480729\n",
            "Epoch: 224. Train loss: 0.00449050. Validation loss: 0.00471278\n",
            "Epoch: 225. Train loss: 0.00448793. Validation loss: 0.00458133\n",
            "Epoch: 226. Train loss: 0.00446936. Validation loss: 0.00476628\n",
            "Epoch: 227. Train loss: 0.00449627. Validation loss: 0.00461447\n",
            "Epoch: 228. Train loss: 0.00448481. Validation loss: 0.00458161\n",
            "Epoch: 229. Train loss: 0.00449085. Validation loss: 0.00478695\n",
            "Epoch: 230. Train loss: 0.00448134. Validation loss: 0.00458507\n",
            "Epoch: 231. Train loss: 0.00448594. Validation loss: 0.00462083\n",
            "Epoch: 232. Train loss: 0.00447649. Validation loss: 0.00459055\n",
            "Epoch: 233. Train loss: 0.00448936. Validation loss: 0.00457424\n",
            "Epoch: 234. Train loss: 0.00447737. Validation loss: 0.00467951\n",
            "Epoch: 235. Train loss: 0.00446049. Validation loss: 0.00473278\n",
            "Epoch: 236. Train loss: 0.00446020. Validation loss: 0.00460768\n",
            "Epoch: 237. Train loss: 0.00447950. Validation loss: 0.00472040\n",
            "Epoch: 238. Train loss: 0.00447090. Validation loss: 0.00463943\n",
            "Epoch: 239. Train loss: 0.00447338. Validation loss: 0.00468097\n",
            "Epoch: 240. Train loss: 0.00446324. Validation loss: 0.00457691\n",
            "Epoch: 241. Train loss: 0.00445718. Validation loss: 0.00458403\n",
            "Epoch: 242. Train loss: 0.00444961. Validation loss: 0.00468198\n",
            "Epoch: 243. Train loss: 0.00446265. Validation loss: 0.00461951\n",
            "Epoch: 244. Train loss: 0.00446168. Validation loss: 0.00462003\n",
            "Epoch: 245. Train loss: 0.00446475. Validation loss: 0.00458954\n",
            "Epoch: 246. Train loss: 0.00444084. Validation loss: 0.00469536\n",
            "Epoch: 247. Train loss: 0.00445457. Validation loss: 0.00470325\n",
            "Epoch: 248. Train loss: 0.00446813. Validation loss: 0.00458271\n",
            "Epoch: 249. Train loss: 0.00444736. Validation loss: 0.00462785\n",
            "Epoch: 250. Train loss: 0.00446181. Validation loss: 0.00465092\n",
            "Epoch: 251. Train loss: 0.00445464. Validation loss: 0.00480806\n",
            "Epoch: 252. Train loss: 0.00445784. Validation loss: 0.00466058\n",
            "Epoch: 253. Train loss: 0.00446542. Validation loss: 0.00459335\n",
            "Epoch: 254. Train loss: 0.00444394. Validation loss: 0.00459988\n",
            "Epoch: 255. Train loss: 0.00444132. Validation loss: 0.00494024\n",
            "Epoch: 256. Train loss: 0.00443621. Validation loss: 0.00471798\n",
            "Epoch: 257. Train loss: 0.00444795. Validation loss: 0.00454866\n",
            "Epoch: 258. Train loss: 0.00443648. Validation loss: 0.00459563\n",
            "Epoch: 259. Train loss: 0.00443400. Validation loss: 0.00453473\n",
            "Epoch: 260. Train loss: 0.00444076. Validation loss: 0.00448720\n",
            "Epoch: 261. Train loss: 0.00441950. Validation loss: 0.00478465\n",
            "Epoch: 262. Train loss: 0.00441880. Validation loss: 0.00477798\n",
            "Epoch: 263. Train loss: 0.00444128. Validation loss: 0.00451881\n",
            "Epoch: 264. Train loss: 0.00443595. Validation loss: 0.00459852\n",
            "Epoch: 265. Train loss: 0.00442478. Validation loss: 0.00455203\n",
            "Epoch: 266. Train loss: 0.00442316. Validation loss: 0.00451154\n",
            "Epoch: 267. Train loss: 0.00441571. Validation loss: 0.00475477\n",
            "Epoch: 268. Train loss: 0.00440942. Validation loss: 0.00462631\n",
            "Epoch: 269. Train loss: 0.00442153. Validation loss: 0.00469512\n",
            "Epoch: 270. Train loss: 0.00441997. Validation loss: 0.00458335\n",
            "Epoch: 271. Train loss: 0.00443069. Validation loss: 0.00462086\n",
            "Epoch: 272. Train loss: 0.00440169. Validation loss: 0.00485884\n",
            "Epoch: 273. Train loss: 0.00442960. Validation loss: 0.00476117\n",
            "Epoch: 274. Train loss: 0.00442344. Validation loss: 0.00455620\n",
            "Epoch: 275. Train loss: 0.00441214. Validation loss: 0.00493262\n",
            "Epoch: 276. Train loss: 0.00439676. Validation loss: 0.00485612\n",
            "Epoch: 277. Train loss: 0.00442438. Validation loss: 0.00457129\n",
            "Epoch: 278. Train loss: 0.00440492. Validation loss: 0.00470413\n",
            "Epoch: 279. Train loss: 0.00438529. Validation loss: 0.00459473\n",
            "Epoch: 280. Train loss: 0.00439872. Validation loss: 0.00473748\n",
            "Epoch: 281. Train loss: 0.00442060. Validation loss: 0.00462650\n",
            "Epoch: 282. Train loss: 0.00440864. Validation loss: 0.00473840\n",
            "Epoch: 283. Train loss: 0.00441343. Validation loss: 0.00467021\n",
            "Epoch: 284. Train loss: 0.00440365. Validation loss: 0.00470082\n",
            "Epoch: 285. Train loss: 0.00440510. Validation loss: 0.00457304\n",
            "Epoch: 286. Train loss: 0.00440056. Validation loss: 0.00452860\n",
            "Epoch: 287. Train loss: 0.00440865. Validation loss: 0.00465442\n",
            "Epoch: 288. Train loss: 0.00439179. Validation loss: 0.00450384\n",
            "Epoch: 289. Train loss: 0.00440055. Validation loss: 0.00457677\n",
            "Epoch: 290. Train loss: 0.00440178. Validation loss: 0.00552876\n",
            "Epoch: 291. Train loss: 0.00440045. Validation loss: 0.00472596\n",
            "Epoch: 292. Train loss: 0.00440145. Validation loss: 0.00461823\n",
            "Epoch: 293. Train loss: 0.00439791. Validation loss: 0.00457883\n",
            "Epoch: 294. Train loss: 0.00439833. Validation loss: 0.00495602\n",
            "Epoch: 295. Train loss: 0.00439779. Validation loss: 0.00452192\n",
            "Epoch: 296. Train loss: 0.00439804. Validation loss: 0.00461556\n",
            "Epoch: 297. Train loss: 0.00439214. Validation loss: 0.00460903\n",
            "Epoch: 298. Train loss: 0.00439018. Validation loss: 0.00451687\n",
            "Epoch: 299. Train loss: 0.00439301. Validation loss: 0.00475387\n",
            "Epoch: 300. Train loss: 0.00438358. Validation loss: 0.00455911\n",
            "Epoch: 301. Train loss: 0.00440495. Validation loss: 0.00455492\n",
            "Epoch: 302. Train loss: 0.00438541. Validation loss: 0.00488839\n",
            "Epoch: 303. Train loss: 0.00438980. Validation loss: 0.00466592\n",
            "Epoch: 304. Train loss: 0.00440025. Validation loss: 0.00453045\n",
            "Epoch: 305. Train loss: 0.00438860. Validation loss: 0.00478924\n",
            "Epoch: 306. Train loss: 0.00439017. Validation loss: 0.00452796\n",
            "Epoch: 307. Train loss: 0.00440086. Validation loss: 0.00458246\n",
            "Epoch: 308. Train loss: 0.00437628. Validation loss: 0.00469794\n",
            "Epoch: 309. Train loss: 0.00438417. Validation loss: 0.00462328\n",
            "Epoch: 310. Train loss: 0.00440294. Validation loss: 0.00463982\n",
            "Epoch: 311. Train loss: 0.00437167. Validation loss: 0.00463528\n",
            "Epoch: 312. Train loss: 0.00438301. Validation loss: 0.00467807\n",
            "Epoch: 313. Train loss: 0.00438823. Validation loss: 0.00457571\n",
            "Epoch: 314. Train loss: 0.00440130. Validation loss: 0.00450961\n",
            "Epoch: 315. Train loss: 0.00440157. Validation loss: 0.00446582\n",
            "Epoch: 316. Train loss: 0.00437959. Validation loss: 0.00468082\n",
            "Epoch: 317. Train loss: 0.00438913. Validation loss: 0.00465522\n",
            "Epoch: 318. Train loss: 0.00438737. Validation loss: 0.00455064\n",
            "Epoch: 319. Train loss: 0.00438655. Validation loss: 0.00446305\n",
            "Epoch: 320. Train loss: 0.00436553. Validation loss: 0.00489006\n",
            "Epoch: 321. Train loss: 0.00436171. Validation loss: 0.00451185\n",
            "Epoch: 322. Train loss: 0.00436871. Validation loss: 0.00460691\n",
            "Epoch: 323. Train loss: 0.00436225. Validation loss: 0.00457103\n",
            "Epoch: 324. Train loss: 0.00435569. Validation loss: 0.00516124\n",
            "Epoch: 325. Train loss: 0.00437785. Validation loss: 0.00470101\n",
            "Epoch: 326. Train loss: 0.00437831. Validation loss: 0.00458343\n",
            "Epoch: 327. Train loss: 0.00434827. Validation loss: 0.00483452\n",
            "Epoch: 328. Train loss: 0.00436311. Validation loss: 0.00481101\n",
            "Epoch: 329. Train loss: 0.00438017. Validation loss: 0.00454306\n",
            "Epoch: 330. Train loss: 0.00435252. Validation loss: 0.00453992\n",
            "Epoch: 331. Train loss: 0.00436924. Validation loss: 0.00453250\n",
            "Epoch: 332. Train loss: 0.00435336. Validation loss: 0.00463057\n",
            "Epoch: 333. Train loss: 0.00437518. Validation loss: 0.00448143\n",
            "Epoch: 334. Train loss: 0.00436284. Validation loss: 0.00455978\n",
            "Epoch: 335. Train loss: 0.00435889. Validation loss: 0.00462280\n",
            "Epoch: 336. Train loss: 0.00436105. Validation loss: 0.00455821\n",
            "Epoch: 337. Train loss: 0.00434848. Validation loss: 0.00450041\n",
            "Epoch: 338. Train loss: 0.00433698. Validation loss: 0.00457698\n",
            "Epoch: 339. Train loss: 0.00437721. Validation loss: 0.00457795\n",
            "Epoch: 340. Train loss: 0.00436431. Validation loss: 0.00464934\n",
            "Epoch: 341. Train loss: 0.00434202. Validation loss: 0.00488899\n",
            "Epoch: 342. Train loss: 0.00437045. Validation loss: 0.00454102\n",
            "Epoch: 343. Train loss: 0.00435130. Validation loss: 0.00461893\n",
            "Epoch: 344. Train loss: 0.00436628. Validation loss: 0.00448701\n",
            "Epoch: 345. Train loss: 0.00436267. Validation loss: 0.00459743\n",
            "Epoch: 346. Train loss: 0.00435544. Validation loss: 0.00457164\n",
            "Epoch: 347. Train loss: 0.00436958. Validation loss: 0.00454169\n",
            "Epoch: 348. Train loss: 0.00436709. Validation loss: 0.00472067\n",
            "Epoch: 349. Train loss: 0.00433140. Validation loss: 0.00456620\n",
            "Epoch: 350. Train loss: 0.00435847. Validation loss: 0.00447467\n",
            "Epoch: 351. Train loss: 0.00435272. Validation loss: 0.00514750\n",
            "Epoch: 352. Train loss: 0.00435004. Validation loss: 0.00453407\n",
            "Epoch: 353. Train loss: 0.00433957. Validation loss: 0.00448207\n",
            "Epoch: 354. Train loss: 0.00435162. Validation loss: 0.00454280\n",
            "Epoch: 355. Train loss: 0.00432260. Validation loss: 0.00472154\n",
            "Epoch: 356. Train loss: 0.00433664. Validation loss: 0.00459113\n",
            "Epoch: 357. Train loss: 0.00431915. Validation loss: 0.00462311\n",
            "Epoch: 358. Train loss: 0.00430835. Validation loss: 0.00451008\n",
            "Epoch: 359. Train loss: 0.00433405. Validation loss: 0.00457557\n",
            "Epoch: 360. Train loss: 0.00432995. Validation loss: 0.00464139\n",
            "Epoch: 361. Train loss: 0.00436074. Validation loss: 0.00452510\n",
            "Epoch: 362. Train loss: 0.00431920. Validation loss: 0.00460151\n",
            "Epoch: 363. Train loss: 0.00433622. Validation loss: 0.00455921\n",
            "Epoch: 364. Train loss: 0.00431667. Validation loss: 0.00448164\n",
            "Epoch: 365. Train loss: 0.00434543. Validation loss: 0.00475409\n",
            "Epoch: 366. Train loss: 0.00433042. Validation loss: 0.00457974\n",
            "Epoch: 367. Train loss: 0.00431013. Validation loss: 0.00466895\n",
            "Epoch: 368. Train loss: 0.00431283. Validation loss: 0.00460317\n",
            "Epoch: 369. Train loss: 0.00432515. Validation loss: 0.00455965\n",
            "Epoch: 370. Train loss: 0.00432564. Validation loss: 0.00453596\n",
            "Epoch: 371. Train loss: 0.00433612. Validation loss: 0.00452148\n",
            "Epoch: 372. Train loss: 0.00430600. Validation loss: 0.00464930\n",
            "Epoch: 373. Train loss: 0.00433487. Validation loss: 0.00456515\n",
            "Epoch: 374. Train loss: 0.00433226. Validation loss: 0.00458636\n",
            "Epoch: 375. Train loss: 0.00430903. Validation loss: 0.00445676\n",
            "Epoch: 376. Train loss: 0.00431354. Validation loss: 0.00458703\n",
            "Epoch: 377. Train loss: 0.00430951. Validation loss: 0.00466952\n",
            "Epoch: 378. Train loss: 0.00431475. Validation loss: 0.00487423\n",
            "Epoch: 379. Train loss: 0.00431907. Validation loss: 0.00466663\n",
            "Epoch: 380. Train loss: 0.00433356. Validation loss: 0.00452579\n",
            "Epoch: 381. Train loss: 0.00432478. Validation loss: 0.00477687\n",
            "Epoch: 382. Train loss: 0.00432781. Validation loss: 0.00453516\n",
            "Epoch: 383. Train loss: 0.00431799. Validation loss: 0.00462617\n",
            "Epoch: 384. Train loss: 0.00432193. Validation loss: 0.00446133\n",
            "Epoch: 385. Train loss: 0.00430850. Validation loss: 0.00460030\n",
            "Epoch: 386. Train loss: 0.00429713. Validation loss: 0.00482532\n",
            "Epoch: 387. Train loss: 0.00429944. Validation loss: 0.00448829\n",
            "Epoch: 388. Train loss: 0.00431122. Validation loss: 0.00446024\n",
            "Epoch: 389. Train loss: 0.00431260. Validation loss: 0.00451677\n",
            "Epoch: 390. Train loss: 0.00431264. Validation loss: 0.00446777\n",
            "Epoch: 391. Train loss: 0.00430761. Validation loss: 0.00444836\n",
            "Epoch: 392. Train loss: 0.00430743. Validation loss: 0.00452762\n",
            "Epoch: 393. Train loss: 0.00430168. Validation loss: 0.00451383\n",
            "Epoch: 394. Train loss: 0.00428926. Validation loss: 0.00454259\n",
            "Epoch: 395. Train loss: 0.00430672. Validation loss: 0.00470048\n",
            "Epoch: 396. Train loss: 0.00429629. Validation loss: 0.00458214\n",
            "Epoch: 397. Train loss: 0.00429871. Validation loss: 0.00442083\n",
            "Epoch: 398. Train loss: 0.00430407. Validation loss: 0.00481536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjURP6ilO-qA"
      },
      "source": [
        "Como visto acima, o erro obtido á da ordem de $10^{-3}$.\n",
        "\n",
        "A curva de aprendizado pode ser vista na saída da célula abaixo. Esta curva tem como eixos as épocas e o valor da função custo na época. É importante para ver a evolução do treinamento e identificar se a rede está passando por *overfitting* ou se o treinamento estagnou, e em qual época isso ocorre."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "skAj8Sen_DLC",
        "outputId": "f15a5c10-16f4-4efd-b882-8430420d839d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=[10, 7])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(train_losses, label=\"Train loss\")\n",
        "plt.plot(valid_losses, label=\"Validation loss\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4938cf32803d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Train loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Validation loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S3GnRQgPnno"
      },
      "source": [
        "Para finalizar, o erro pode ser calculado também no conjunto de teste e a rede pode ser salva num arquivo .pth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8rIVIqyrxAU"
      },
      "source": [
        "# test the network\n",
        "\n",
        "out_array = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  test_loss = []\n",
        "  for x in test_dataset:\n",
        "    output = model(x)\n",
        "    out_array.append(output.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3_XIxskjfpk"
      },
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/My Drive/mlp-optimized.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
